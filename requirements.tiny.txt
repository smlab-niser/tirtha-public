# Linting and testing
black
flake8
isort
tox
rich

# Django + Celery + Gunicorn
amqp
celery
Django
django-cleanup
django-dbbackup
django-extensions
gunicorn
kombu
psycopg2
psycopg2-binary
pytz
pytzdata
shortuuid

# For Google Auth
google-auth
google-cloud-vision

# For the NN models
jax
keras
# -e git+https://github.com/GantMan/nsfw_model.git@699b6796a55604341fbfdffe2b27ced1d868c591#egg=nsfw_detector
# protobuf==3.20.3
silence-tensorflow
tensorflow
torch
timm

# Basic requirements
numpy
opencv-python
pandas
scipy
Pillow
pillow-heif



# For training and prototyping my lightweight models, I will use Pytorch. Efficient architectures MobileNet, EfficientNet-lite and other similar architectures are natively written in pytorch for vision-based tasks and will be instrumental in monitoring the driver behavior, as well as time-series models for predicting driver behavior based on vehicle telemetry. In addition, I will leverage the Hugging Face Transformers library for experimenting with transformer-based architectures, including Linformer and DistilBERT, which can effectively capture complex temporal dependencies while remaining more efficient than traditional self-attention models.

# Once the models are trained, I will convert them to the ONNX (Open Neural Network Exchange) format, which serves as a common intermediate representation. This allows me to deploy the same model across different runtimes and hardware accelerators. For optimization, I will rely on ONNX Runtime for post-training quantization and performance tuning. Most probably NVIDIA platforms such as the Jetson Xavier or AGX Orin will be targeted, which means these models have space for further optimization with NVIDIA TensorRT, which supports layer fusion and FP16/INT8 quantization to reduce memory footprint and improve inference speed. Aiming to make the solution platform-independent OpenVINO acceleration for Intel-based systems shall also be incorporated. 

# Since many of my models involve transformer architectures, I will incorporate specialized optimization techniques to reduce their computational and memory overhead. I plan to experiment with Linformer, which approximates self-attention in linear time, and leverage ONNX Runtime Transformers to take advantage of optimized kernels for attention and layer normalization. Additionally, I will explore TVM, an open-source compiler stack that can generate highly optimized transformer operators tailored to my target hardware. During training, I may also incorporate DeepSpeed or FairScale to enable quantization-aware fine-tuning and memory-efficient training of large models.

# For storing and transmitting sparse data—such as gaze heatmaps, occupancy grids, and other partially populated tensors—I will rely on SciPy Sparse and PyTorch Sparse to represent tensors in COO, CSR, or CSC formats. This ensures that only nonzero elements are serialized and transferred between components. When capturing and archiving image data locally, I will encode it using efficient codecs like WebP and HEIF, leveraging libraries such as libwebp and libheif to compress data without significant quality loss. For compact, high-speed serialization of sparse tensors or metadata, I will also consider Cap’n Proto or FlatBuffers.

# Having previously worked with ROS systems, I will build my middleware stack around ROS 2, which uses DDS (Data Distribution Service) as the communication backbone to facilitate low-latency, zero-copy data sharing between the cameras, sensors, and central compute unit. Specifically, I plan to use Fast DDS or Cyclone DDS depending on performance benchmarks on my hardware platform. To avoid unnecessary memory copies when transmitting large sensor messages, I will integrate Iceoryx, which provides true zero-copy transport by placing data into shared memory accessible by multiple nodes. For capturing and preprocessing video streams efficiently, I will set up pipelines with GStreamer, potentially interfacing it with OpenCV to perform real-time transformations and normalization before passing data to the inference models.

# This modular combination of tools will help me train efficient, transformer-based driver behavior monitoring models, compress and quantize them for deployment, represent sparse data compactly, and orchestrate fast data sharing across components while maintaining low latency and high throughput. With this framework, I can ensure the entire system remains performant, scalable, and suitable for edge deployment in automotive environments.